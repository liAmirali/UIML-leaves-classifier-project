# -*- coding: utf-8 -*-
"""MLFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sq1bMobtq8GA8BUSgyIrG3fn5ToARViW
"""

import numpy as np
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
from scipy.stats import norm, randint

!pip install --quiet yellowbrick
from yellowbrick.cluster import KElbowVisualizer

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, OPTICS
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay, silhouette_score, pairwise_distances

# Tree Visualisation
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

!pip install scikit-image
!pip install opencv-python

import os
import cv2
from skimage.feature import hog, local_binary_pattern
from skimage import color

df0 = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/MLFinal/leaves.csv", header=None)

# Resetting header
columns_names = []
for i in range(df0.shape[1]):
  if i == 0:
    columns_names.append("Leaf_Category")
  elif i == 1:
    columns_names.append("Img_Index")
  else:
    columns_names.append(f"Feature_{i-2}")
df0.columns = columns_names

# Extracting Class Labels
class_labels = set(df0["Leaf_Category"])
class_labels

df0

# Define parameters for LBP
radius = 1
n_points = 8 * radius
METHOD = 'uniform'

# Function to extract HOG and LBP features
def extract_features(image):
    # Convert image to grayscale
    gray_image = color.rgb2gray(image)

    # Extract HOG features
    hog_features, hog_image = hog(
        gray_image,
        orientations=8,
        pixels_per_cell=(16, 16),
        cells_per_block=(1, 1),
        visualize=True,
        multichannel=False
    )

    # Extract LBP features
    lbp = local_binary_pattern(gray_image, n_points, radius, METHOD)
    (lbp_hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))
    lbp_hist = lbp_hist.astype("float")
    lbp_hist /= (lbp_hist.sum() + 1e-6)  # Normalize the histogram

    # Extract Color Histogram features (for each channel)
    chans = cv2.split(image)
    colors = ("b", "g", "r")
    hist_features = []
    for (chan, clr) in zip(chans, colors):
        hist = cv2.calcHist([chan], [0], None, [256], [0, 256])
        hist = cv2.normalize(hist, hist).flatten()
        hist_features.extend(hist)

    return hog_features, lbp_hist, hist_features

# Initialize lists to store features and labels
hog_features_list = []
lbp_features_list = []
hist_features_list = []
image_labels_list = []

# Path to the images directory
images_dir = '/content/drive/MyDrive/Colab Notebooks/MLFinal/leaves'

# Read images and extract features
category_list = os.listdir(images_dir)
for dir_name in category_list:
    dot_i = dir_name.index('. ')
    cat_index = int(dir_name[:dot_i])
    if cat_index not in class_labels:
      continue

    category_path = os.path.join(images_dir, dir_name)
    if os.path.isdir(category_path):
        for img_i, img_name in enumerate(os.listdir(category_path)):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB

            # Extract features
            hog_features, lbp_features, hist_features = extract_features(image)

            # Append features and label
            hog_features_list.append(hog_features)
            lbp_features_list.append(lbp_features)
            hist_features_list.append(hist_features)
            image_labels_list.append((cat_index, img_i + 1))

# Convert lists to numpy arrays
hog_features_array = np.array(hog_features_list)
lbp_features_array = np.array(lbp_features_list)
hist_features_array = np.array(hist_features_list)
labels_array = np.array(image_labels_list)

# Create a DataFrame with the features and labels
hog_features_df = pd.DataFrame(hog_features_array, columns=['hog_'+str(i) for i in range(hog_features_array.shape[1])])
lbp_features_df = pd.DataFrame(lbp_features_array, columns=['lbp'+str(i) for i in range(lbp_features_array.shape[1])])
hist_features_df = pd.DataFrame(hist_features_array, columns=['hist_'+str(i) for i in range(hist_features_array.shape[1])])
img_labels_df = pd.DataFrame(labels_array, columns=['Leaf_Category', 'Img_Index'])

# Concatenate extracted features
img_features_df = pd.concat([img_labels_df, lbp_features_df], axis=1)

img_features_df = img_features_df.sort_values(['Leaf_Category', 'Img_Index'], ignore_index=True)

img_features_df

df0_img = pd.merge(left=df0, right=img_features_df, how='inner')

df0_img

df0.describe()

df0.info()

df0.value_counts()

# Count the occurrences of each value in the column 'Feature1'
cat_len = df0["Leaf_Category"].value_counts()

# Plot the occurrences
plt.figure(figsize=(8, 6))
cat_len.sort_index().plot(kind='bar', color='skyblue')
plt.xlabel('Category')
plt.ylabel('Occurrences')
plt.title('Number of Items of Each Category')
plt.show()

X = df0_img.drop(columns=["Leaf_Category", "Img_Index"])
y = df0_img["Leaf_Category"]

Xcsv = df0.drop(columns=["Leaf_Category", "Img_Index"])

y.value_counts()

# Plot distributions for all features
num_features = Xcsv.shape[1]
num_subplots_per_row = 3
num_rows = (num_features + num_subplots_per_row - 1) // num_subplots_per_row

fig, axes = plt.subplots(nrows=num_rows, ncols=num_subplots_per_row, figsize=(15, 3*num_rows))

for i, column in enumerate(Xcsv.columns):
    row, col = divmod(i, num_subplots_per_row)
    sb.histplot(Xcsv[column], kde=False, stat="density", color='skyblue', ax=axes[row, col], label='Data Distribution')
    mu, std = norm.fit(Xcsv[column])
    xmin, xmax = axes[row, col].get_xlim()
    x = np.linspace(xmin, xmax, 100)
    p = norm.pdf(x, mu, std)
    axes[row, col].plot(x, p, 'k', linewidth=2, label='Normal Distribution')
    axes[row, col].set_xlabel('Value')
    axes[row, col].set_ylabel('Density')
    axes[row, col].set_title(f'Distribution of {column} and Normal Distribution Fit')
    axes[row, col].legend()

plt.tight_layout()
plt.show()

# Compute the correlation matrix
corr = Xcsv.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Draw the heatmap with the mask and correct aspect ratio
sb.heatmap(corr, mask=mask, center=0,
            square=True, linewidths=.5, annot=True)

def dunn_index(X, labels):
    """
    Calculate the Dunn index for cluster validity.

    Parameters:
    X : ndarray of shape (n_samples, n_features)
        Data points.
    labels : ndarray of shape (n_samples,)
        Cluster labels for each data point.

    Returns:
    float
        Dunn index.
    """
    unique_labels = np.unique(labels)
    num_clusters = len(unique_labels)

    if num_clusters < 2:
        raise ValueError("There must be at least two clusters to compute the Dunn index.")

    # Compute pairwise distances between all points
    distances = pairwise_distances(X)

    # Compute intra-cluster distances (max distance within a cluster)
    intra_cluster_distances = np.zeros(num_clusters)
    for k in unique_labels:
        cluster_k = X[labels == k]
        if cluster_k.shape[0] > 1:
            intra_cluster_distances[k] = np.max(pairwise_distances(cluster_k))
        else:
            intra_cluster_distances[k] = 0

    # Compute inter-cluster distances (min distance between clusters)
    inter_cluster_distances = np.full((num_clusters, num_clusters), np.inf)
    for i, label_i in enumerate(unique_labels):
        for j, label_j in enumerate(unique_labels):
            if label_i != label_j:
                cluster_i = X[labels == label_i]
                cluster_j = X[labels == label_j]
                inter_cluster_distances[i, j] = np.min(pairwise_distances(cluster_i, cluster_j))

    min_inter_cluster_distance = np.min(inter_cluster_distances)
    max_intra_cluster_distance = np.max(intra_cluster_distances)

    dunn_score = min_inter_cluster_distance / max_intra_cluster_distance
    return dunn_score

km = KMeans()
visualizer = KElbowVisualizer(km, k=(2,37))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer.show()        # Finalize and render the figure

# Clustering
kmeans = KMeans(n_clusters=9)
clusters = kmeans.fit_predict(X)
silhouette = silhouette_score(X, clusters)
dunn = dunn_index(X, clusters)
print(f"{silhouette=}")
print(f"{dunn=}")

df0_img['cluster'] = clusters

# Create a contingency table of cluster labels vs leaf categories
contingency_table = pd.crosstab(df0_img['cluster'], df0_img['Leaf_Category'])

# Plot stacked bar chart
contingency_table.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='tab20')
plt.xlabel('cluster')
plt.ylabel('Number of Samples')
plt.title('Distribution of Leaf Categories in Each cluster')
plt.legend(title='Leaf Category', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Plot heatmap
plt.figure(figsize=(12, 8))
sb.heatmap(contingency_table, annot=True, cmap='viridis', fmt='d')
plt.xlabel('Leaf Category')
plt.ylabel('Cluster')
plt.title('Heatmap of Leaf Categories in Each Cluster')
plt.show()

X

X = pd.get_dummies(X, columns=['cluster'], drop_first=True, dtype=int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Check the sizes
print(f'Training set size: {X_train.shape}')
print(f'Test set size: {X_test.shape}')

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

rf = RandomForestClassifier(n_estimators=500, max_depth=18)
rf.fit(X_train, y_train)

# Predict on validation and test sets
test_preds = rf.predict(X_test)

# Calculate accuracy
test_accuracy = accuracy_score(y_test, test_preds)

print(f'Test Accuracy: {test_accuracy}')

param_dist = {'n_estimators': randint(50,700),
              'max_depth': randint(4,30)}

# Create a random forest classifier
rf = RandomForestClassifier(criterion='entropy')

# Use random search to find the best hyperparameters
rand_search = RandomizedSearchCV(rf,
                                 param_distributions = param_dist,
                                 n_iter=20,
                                 cv=5)

# Fit the random search object to the data
rand_search.fit(X_train, y_train)

# Create a variable for the best model
best_rf = rand_search.best_estimator_

# Print the best hyperparameters
print('Best hyperparameters:',  rand_search.best_params_)

y_pred = best_rf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("Accuracy:", accuracy)

y_pred

log_reg = LogisticRegression(max_iter=10000, multi_class='ovr')
log_reg.fit(X_train, y_train)
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

